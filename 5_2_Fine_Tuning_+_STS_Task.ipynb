{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qP5_hiG_WnM7SN8XL06tSUqnoiHxXOoc",
      "authorship_tag": "ABX9TyOuGdCFV5I7C9qtQVYanN6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asato4931/CA_5_2/blob/main/5_2_Fine_Tuning_%2B_STS_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZfKmdVl5Gr0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import models\n",
        " \n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "from sentence_transformers.losses import TripletDistanceMetric, TripletLoss\n",
        "from sentence_transformers.evaluation import TripletEvaluator\n",
        "from sentence_transformers.readers import TripletReader\n",
        "from sentence_transformers.datasets import SentencesDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from sentence_transformers.losses import InputExample, losses\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load pre-trained tokenizer\n",
        "transformer = models.Transformer(\"cl-tohoku/bert-base-japanese-v2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
        "#model = AutoModelForMaskedLM.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
        "pooling = models.Pooling(transformer.get_word_embedding_dimension(), pooling_mode_mean_tokens=True, pooling_mode_cls_token=False, pooling_mode_max_tokens=False)\n",
        "model = SentenceTransformer(modules=[transformer, pooling])\n",
        "\n",
        "\n",
        "\n",
        "train_data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/CA課題用/CA課題5/train-v1.1.json', lines=True)\n",
        "\n",
        "\n",
        "train_data = train_data.loc[:,['sentence1','sentence2']]\n",
        "train_label = train_data.loc[:,['label']]\n",
        "train_label = train_label / 5\n",
        "\n",
        "pearson_train = []\n",
        "\n",
        "\n",
        "\n",
        "#Pearson\n",
        "\n",
        "for i in range(0,10):\n",
        "  vector1_pt = model.encode(train_data.loc[i,'sentence1'])\n",
        "  vector2_pt = model.encode(train_data.loc[i,'sentence2'])\n",
        "\n",
        "  result_pt= stats.pearsonr(vector1_pt,vector2_pt)\n",
        "  pearson_train.append(result_pt.statistic)\n",
        "\n",
        "\n",
        "pearson_train_df = pd.DataFrame(pearson_train)\n",
        "pearson_train_df[\"metrics\"] = \"pearson\"\n",
        "pearson_train_df = pearson_train_df.rename(columns= { pearson_train_df.columns[0]: \"score\"})\n",
        "pearson_train_df = pearson_train_df.reindex(columns=['metrics', 'score'])\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 1\n",
        "EVAL_STEPS = 1000\n",
        "WARMUP_STEPS = int(len(train_data) // BATCH_SIZE * 0.1) \n",
        "#OUTPUT_PATH = \"./sbert_stair\"\n",
        "\n",
        "\n",
        "\n",
        "triplet_reader = TripletReader(\".\")\n",
        "train_data = SentencesDataset(triplet_reader.get_examples('triplet_train.tsv'), model=model)\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "train_loss = CosineSimilarityLoss(model=model)\n",
        "\n",
        "dev_data = SentencesDataset(triplet_reader.get_examples('triplet_dev.tsv'), model=model)\n",
        "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=BATCH_SIZE)\n",
        "evaluator = TripletEvaluator(dev_dataloader)\n",
        "\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],evaluator=evaluator,epochs=NUM_EPOCHS,evaluation_steps=EVAL_STEPS,warmup_steps=WARMUP_STEPS)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "valid_data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/CA課題用/CA課題5/valid-v1.1.json', lines=True)\n",
        "\n",
        "\n",
        "valid_data = valid_data.loc[:,['sentence1','sentence2']]\n",
        "\n",
        "\n",
        "\n",
        "pearson_val = []\n",
        "spearman_val = []\n",
        "\n",
        "\n",
        "\n",
        "#Pearson\n",
        "\n",
        "for i in range(0,1457):\n",
        "  vector1_pv = model.encode(valid_data.loc[i,'sentence1'])\n",
        "  vector2_pv = model.encode(valid_data.loc[i,'sentence2'])\n",
        "\n",
        "  result_pv= stats.pearsonr(vector1_pv,vector2_pv)\n",
        "  pearson_val.append(result_pv.statistic)\n",
        "\n",
        "\n",
        "#Spearman\n",
        "\n",
        "for i in range(0,1457):\n",
        "  vector1_s = model.encode(valid_data.loc[i,'sentence1'])\n",
        "  vector2_s = model.encode(valid_data.loc[i,'sentence2'])\n",
        "\n",
        "  vector1_s = np.array(vector1_s).reshape(1,-1)\n",
        "  vector2_s = np.array(vector2_s).reshape(1,-1)\n",
        "  result_s = stats.spearmanr(vector1_s,vector2_s ,axis=None)\n",
        "  spearman_val.append(result_s.statistic)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pearson_val_df = pd.DataFrame(pearson_val)\n",
        "pearson_val_df = pearson_val_df * 5\n",
        "pearson_val_df[\"metrics\"] = \"pearson\"\n",
        "pearson_val_df = pearson_val_df.rename(columns= { pearson_val_df.columns[0]: \"score\"})\n",
        "pearson_val_df = pearson_val_df.reindex(columns=['metrics', 'score'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "spearman_val_df = pd.DataFrame(spearman_val)\n",
        "spearman_val_df = spearman_val_df * 5\n",
        "spearman_val_df[\"metrics\"] = \"spearman\"\n",
        "spearman_val_df = spearman_val_df.rename(columns= { spearman_val_df.columns[0]: \"score\"})\n",
        "spearman_val_df = spearman_val_df.reindex(columns=['metrics', 'score'])\n",
        "\n",
        "\n",
        "frames = [pearson_val_df, spearman_val_df]\n",
        "\n",
        "result_5_2 = pd.concat(frames)\n",
        "\n",
        "result_5_2.to_json('/content/drive/MyDrive/Colab Notebooks/CA課題用/CA課題5/CA_5_2 STS Task Results.jsonl', force_ascii=False, lines = True, orient='records' )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "\n",
        "!apt-get install mecab mecab-ipadic-utf8 python-mecab libmecab-dev\n",
        "!pip install mecab-python3 \n",
        "\n",
        "!pip install fugashi\n",
        "\n",
        "!pip install unidic_lite"
      ],
      "metadata": {
        "id": "WTut2Yqcs-WC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}